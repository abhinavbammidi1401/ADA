{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVN8HRAOPlsNWceV3b1mRT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhinavbammidi1401/ADA/blob/main/Text_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oIeNL2EwyrnC"
      },
      "outputs": [],
      "source": [
        "text = ''' Hey there! ðŸ˜ƒ I can't believe it's already 2024. Did you see John's new blog post? Check it out at https://example.com/blog! Also, email me at john.doe@example.com. He mentioned something about \"stemming\" and \"lemmatization\"â€”interesting stuff. BTW, I'll be attending the AI conference in N.Y.C. next month!! #Excited #AI ðŸ˜Š Let's catch up soon. Cheers, John'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "0VBOSXnHzDJS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "C-oMnifY0SxN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  text = text.lower()\n",
        "\n",
        "  text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "  text = re.sub(r'https\\S+|www\\S+http\\S+', '', text)\n",
        "\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "  stemmer = PorterStemmer()\n",
        "  tokens_stemmed = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens_lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "  doc = nlp(' '.join(tokens))\n",
        "  tokens_spacy_lemmatized = [token.lemma_ for token in doc]\n",
        "\n",
        "  return{\n",
        "      'original': text,\n",
        "      'tokens': tokens,\n",
        "      'stemmed': tokens_stemmed,\n",
        "      'lemmatized': tokens_lemmatized,\n",
        "      'spacy_lemmatized': tokens_spacy_lemmatized\n",
        "  }"
      ],
      "metadata": {
        "id": "fy8ExLE80htO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = preprocess_text(text)"
      ],
      "metadata": {
        "id": "x8wmFL2Y12RW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Text:\")\n",
        "print(preprocessed_text['original'])\n",
        "print(\"\\nTokens:\")\n",
        "print(preprocessed_text['tokens'])\n",
        "print(\"\\nStemmed Tokens:\")\n",
        "print(preprocessed_text['stemmed'])\n",
        "print(\"\\nLemmatized Tokens:\")\n",
        "print(preprocessed_text['lemmatized'])\n",
        "print(\"\\nSpacy Lemmatized Tokens:\")\n",
        "print(preprocessed_text['spacy_lemmatized'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlxHuqy714Mq",
        "outputId": "892e92da-a89b-4b77-aae0-225e2fffce3a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " hey there  i cant believe its already 2024 did you see johns new blog post check it out at  also email me at  he mentioned something about stemming and lemmatizationinteresting stuff btw ill be attending the ai conference in nyc next month excited ai  lets catch up soon cheers john\n",
            "\n",
            "Tokens:\n",
            "['hey', 'cant', 'believe', 'already', '2024', 'see', 'johns', 'new', 'blog', 'post', 'check', 'also', 'email', 'mentioned', 'something', 'stemming', 'lemmatizationinteresting', 'stuff', 'btw', 'ill', 'attending', 'ai', 'conference', 'nyc', 'next', 'month', 'excited', 'ai', 'lets', 'catch', 'soon', 'cheers', 'john']\n",
            "\n",
            "Stemmed Tokens:\n",
            "['hey', 'cant', 'believ', 'alreadi', '2024', 'see', 'john', 'new', 'blog', 'post', 'check', 'also', 'email', 'mention', 'someth', 'stem', 'lemmatizationinterest', 'stuff', 'btw', 'ill', 'attend', 'ai', 'confer', 'nyc', 'next', 'month', 'excit', 'ai', 'let', 'catch', 'soon', 'cheer', 'john']\n",
            "\n",
            "Lemmatized Tokens:\n",
            "['hey', 'cant', 'believe', 'already', '2024', 'see', 'john', 'new', 'blog', 'post', 'check', 'also', 'email', 'mentioned', 'something', 'stemming', 'lemmatizationinteresting', 'stuff', 'btw', 'ill', 'attending', 'ai', 'conference', 'nyc', 'next', 'month', 'excited', 'ai', 'let', 'catch', 'soon', 'cheer', 'john']\n",
            "\n",
            "Spacy Lemmatized Tokens:\n",
            "['hey', 'can', 'not', 'believe', 'already', '2024', 'see', 'johns', 'new', 'blog', 'post', 'check', 'also', 'email', 'mention', 'something', 'stem', 'lemmatizationintereste', 'stuff', 'btw', 'ill', 'attend', 'ai', 'conference', 'nyc', 'next', 'month', 'excited', 'ai', 'let', 'catch', 'soon', 'cheer', 'john']\n"
          ]
        }
      ]
    }
  ]
}